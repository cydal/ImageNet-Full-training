# ResNet Strikes Back (A2) Training Recipe
# Based on: https://arxiv.org/abs/2110.00476
# Target: ~80.4% top-1 accuracy on ImageNet with ResNet-50

# Data settings
data_root: /fsx/ns1  # FSx mount point
img_size: 224
batch_size: 256  # Per GPU (will be scaled for multi-GPU)
num_workers: 8
pin_memory: true
persistent_workers: true

# Model settings
model_name: resnet50
pretrained: false
num_classes: 1000

# Image preprocessing (ImageNet standard)
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# Training hyperparameters (ResNet Strikes Back A2 recipe)
epochs: 600  # Extended training (A2 uses 600 epochs)
optimizer: sgd
lr: 0.5  # Base LR for batch_size=256, scale linearly for larger batches
momentum: 0.9
weight_decay: 0.00002  # Reduced from standard 1e-4 (2e-5)
lr_scheduler: cosine
warmup_epochs: 5

# Mixed precision training
precision: 16-mixed  # Faster training with minimal accuracy loss

# Augmentation (ResNet Strikes Back recipe)
random_crop: true
random_horizontal_flip: true
auto_augment: null  # No AutoAugment in base recipe
mixup_alpha: 0.2  # Mixup
cutmix_alpha: 1.0  # CutMix (stronger than mixup)
label_smoothing: 0.1  # Label smoothing

# Regularization
use_ema: false  # Not used in base ResNet Strikes Back
gradient_clip_val: null

# Logging
log_every_n_steps: 100
val_check_interval: 1.0  # Validate every epoch

# Checkpointing
save_top_k: 5
monitor: val/acc1
mode: max

# Distributed training (for multi-GPU/multi-node)
strategy: ddp
sync_batchnorm: true

# Reproducibility
seed: 42
deterministic: false  # Set to true for reproducibility (slower)

# Notes:
# - This is the A2 recipe from ResNet Strikes Back
# - Expected accuracy: ~80.4% top-1 on ImageNet
# - Training time: ~600 epochs (longer than standard 90)
# - For faster training, use 90-100 epochs with slightly lower accuracy (~78-79%)
# - LR scaling: If using larger batch size, scale LR linearly
#   Example: batch_size=512 -> lr=1.0, batch_size=1024 -> lr=2.0
