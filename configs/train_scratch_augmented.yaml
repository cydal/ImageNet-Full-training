# Training from Scratch with Full Augmentation
# Using correctly labeled validation data (train split)
# LR finder suggested: 0.501

data_root: /mnt/nvme_data/imagenet_subset_trainonly
img_size: 224

batch_size: 256
num_workers: 8
pin_memory: true
persistent_workers: true
prefetch_factor: 2

model_name: resnet50
pretrained: false  # Train from scratch
num_classes: 100
compile_model: false

mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# Training hyperparameters - TUNED
epochs: 50
optimizer: sgd
lr: 0.5  # LR finder suggested 0.501, using 0.5 for simplicity
momentum: 0.9
weight_decay: 0.00002  # ResNet Strikes Back value
lr_scheduler: cosine
warmup_epochs: 5
max_epochs: 50

precision: 16-mixed

# Full augmentation (ResNet Strikes Back recipe)
random_crop: true
random_horizontal_flip: true
auto_augment: rand-m9-mstd0.5-inc1  # RandAugment with magnitude 9
mixup_alpha: 0.2  # Mixup augmentation
cutmix_alpha: 1.0  # CutMix augmentation
label_smoothing: 0.1  # Label smoothing

use_ema: false
gradient_clip_val: null

log_every_n_steps: 20
val_check_interval: 1.0

save_top_k: 3
monitor: val/acc1
mode: max

devices: 1
strategy: auto
sync_batchnorm: false

seed: 42
deterministic: false

# Expected results with correct validation data:
# - Should reach 70-80% val accuracy on 100 classes
# - Train and val should improve together
# - No massive train/val gap
# - Augmentation will slow down training slightly but improve generalization
