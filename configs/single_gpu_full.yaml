# Single GPU Full Training Configuration
# Practical config for testing full training pipeline before scaling to multi-node
# Based on ResNet Strikes Back principles but with shorter training time

# Data settings
data_root: /mnt/s3-imagenet/imagenet  # S3 mount point with ImageNet (train/ and val/ subdirectories)
img_size: 224
batch_size: 256  # Balanced for A10G (23GB) without OOM
num_workers: 12  # Balanced: 12 workers * 3 prefetch * 256 batch * 230MB ≈ 8GB RAM
pin_memory: true
persistent_workers: true
prefetch_factor: 3  # Conservative: 12*3*256*230MB ≈ 8GB RAM buffer

# Model settings
model_name: resnet50
pretrained: false
num_classes: 1000
compile_model: true  # PyTorch 2.0+ compilation for speed

# Image preprocessing (ImageNet standard)
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# Training hyperparameters
epochs: 100  # Shorter than full 600, but enough to verify pipeline
optimizer: sgd
lr: 0.5  # Scaled for batch_size=256 (0.5 * 256/256)
momentum: 0.9
weight_decay: 0.00002  # ResNet Strikes Back value (2e-5)
lr_scheduler: cosine
warmup_epochs: 5
max_epochs: 100  # For LR scheduler

# Mixed precision training (essential for speed)
precision: 16-mixed  # bf16-mixed if using A100/H100

# Augmentation (ResNet Strikes Back recipe)
random_crop: true
random_horizontal_flip: true
auto_augment: null
mixup_alpha: 0.0  # Disabled for speed (can re-enable later)
cutmix_alpha: 0.0  # Disabled for speed (can re-enable later)
label_smoothing: 0.1

# Regularization
use_ema: false
gradient_clip_val: null

# Logging
log_every_n_steps: 50
val_check_interval: 1.0  # Validate every epoch

# Checkpointing
save_top_k: 3
monitor: val/acc1
mode: max

# Single GPU settings
devices: 1
strategy: auto  # Will use single device strategy
sync_batchnorm: false  # Not needed for single GPU

# Reproducibility
seed: 42
deterministic: false

# Expected results after 100 epochs:
# - Top-1 accuracy: ~77-78%
# - Training time: ~7-10 days on single V100/A100
# - Good enough to verify pipeline before multi-node scaling
