# LR Finder Configuration - Training from Scratch
# Using correctly labeled validation data (train split)
# With full augmentation enabled

data_root: /mnt/nvme_data/imagenet_subset_trainonly
img_size: 224

batch_size: 256
num_workers: 8
pin_memory: true
persistent_workers: true
prefetch_factor: 2

model_name: resnet50
pretrained: false  # Train from scratch
num_classes: 100
compile_model: false

mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# Training hyperparameters for LR finder
epochs: 1
optimizer: sgd
lr: 0.1  # Will be overridden by LR finder
momentum: 0.9
weight_decay: 0.00002
lr_scheduler: none  # Disable for LR finder
warmup_epochs: 0
max_epochs: 1

precision: 16-mixed

# Full augmentation (ResNet Strikes Back recipe)
random_crop: true
random_horizontal_flip: true
auto_augment: rand-m9-mstd0.5-inc1  # RandAugment
mixup_alpha: 0.2  # Mixup
cutmix_alpha: 1.0  # CutMix
label_smoothing: 0.1

use_ema: false
gradient_clip_val: null

log_every_n_steps: 20
val_check_interval: 1.0

save_top_k: 0  # Don't save checkpoints during LR finding
monitor: val/acc1
mode: max

devices: 1
strategy: auto
sync_batchnorm: false

seed: 42
deterministic: false
