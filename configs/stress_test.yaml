# Stress Test Configuration
# Fast iteration on subset to validate training pipeline and tune hyperparameters
# Maximizes GPU utilization while staying within system RAM limits

# Data settings - LOCAL SUBSET for fast testing
data_root: /mnt/nvme_data/imagenet_subset
img_size: 224
# Note: Local data is already filtered to 100 classes, 500 samples/class
# No need to specify max_classes or max_samples_per_class

# Batch size optimized for A10G (23GB GPU)
# Note: Compiled model uses ~18GB, leaving ~5GB for batch
batch_size: 256  # Without compilation, we can use larger batches
num_workers: 8  # Reduced workers to save system RAM
pin_memory: true
persistent_workers: true
prefetch_factor: 2  # Conservative: 8*2*512*230MB â‰ˆ 1.8GB RAM buffer

# Model settings
model_name: resnet50
pretrained: false
num_classes: 100  # Will be overridden by datamodule
compile_model: false  # Disable compilation to save memory and speed up startup

# Image preprocessing (ImageNet standard)
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]

# Training hyperparameters - TUNED with LR finder
epochs: 10  # Quick test - 10 epochs should be enough to see learning
optimizer: sgd
lr: 0.357  # LR finder suggested: 0.357 (was 0.5)
momentum: 0.9
weight_decay: 0.00002  # ResNet Strikes Back value
lr_scheduler: cosine
warmup_epochs: 1  # Short warmup for quick test
max_epochs: 10

# Mixed precision training
precision: 16-mixed  # FP16 for speed and memory efficiency

# Augmentation (minimal for speed)
random_crop: true
random_horizontal_flip: true
auto_augment: null
mixup_alpha: 0.0
cutmix_alpha: 0.0
label_smoothing: 0.1

# Regularization
use_ema: false
gradient_clip_val: null

# Logging - more frequent for quick feedback
log_every_n_steps: 10
val_check_interval: 1.0  # Validate every epoch

# Checkpointing
save_top_k: 2
monitor: val/acc1
mode: max

# Single GPU settings
devices: 1
strategy: auto
sync_batchnorm: false

# Reproducibility
seed: 42
deterministic: false

# Expected results after 10 epochs on 100 classes:
# - Top-1 accuracy: ~60-70% (100-way classification is easier than 1000-way)
# - Training time: ~10-15 minutes per epoch
# - GPU utilization: Should be 80-100%
# - GPU memory: ~18-20GB
# - System RAM: ~10-12GB (safe for 62GB system)
